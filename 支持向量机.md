# 支持向量机

支持向量机`（support vector machines, SVM）`是一种二分类模型，它的基本模型是定义在特征空间上的**间隔最大的线性分类器**，间隔最大使它有别于感知机；`SVM`还包括**核技巧**，这使它成为实质上的非线性分类器。`SVM`的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。`SVM`的的学习算法就是求解凸二次规划的最优化算法。

`SVM`学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面

> 简单理解：对于线性可分的数据集，可划分的超平面可以有无数个，比如
>
> ![img](https://syisbest.github.io/svm/7.png)
>
> 可以找到无数条直线把数据集分开，那从这些可分离的超平面中找到一个几何间隔最大的分离超平面就是`SVM`做的事

## 间隔与支持向量

### 几何间隔

几何间隔是点到超平面的距离，在二维平面理解，就是点到直线的距离

二维平面，点到直线距离公式：

设直线 L 的方程为`Ax+By+C=0`，点 P 的坐标为`(Xo，Yo)`，则点 P 到直线 L 的距离为：

![img](https://bkimg.cdn.bcebos.com/formula/9952cb1396c94cf95da20fcc455d28ad.svg)

由此公式可以扩展到多维，超平面方程为：

![img](https://syisbest.github.io/svm/1.png)

这里的w就是`(w1,w2,w3,...,wn)`，x是`(x1,x2,x3,...,xn)`

由二维情况的方程很容易推导出多维情况点到超平面距离的公式：

![img](https://syisbest.github.io/svm/2.png)

### 最大间隔

最大间隔分类器的目标函数定义为：

![img](https://syisbest.github.io/svm/3.png)

> 疑惑：该函数的推导和意义

对于`y(w'x+b)=1`的数据点，即下图中位于`w'x+b=1`或`w'x+b=-1`上的数据点，我们称之为**支持向量**（support vector），易知：对于所有的支持向量，它们恰好满足`y*(w'x*+b)=1`，而所有不是支持向量的点，有`y*(w'x*+b)>1`。

![9.png](https://camo.githubusercontent.com/fb2b56682fa052c622c39089f781f920dd04f794/68747470733a2f2f692e6c6f6c692e6e65742f323031382f31302f31372f356263373266366138333863342e706e67)

求`1/||w||`最大值即求`1/2*||w||^2`的最小值，即转换为：

![img](https://syisbest.github.io/svm/4.png)

这个式子就是`SVM`的基本型

> 这里已经晕了

## 对偶问题

对偶问题，顾名思义，可以理解成优化等价的问题，更一般地，是将一个原始目标函数的最小化转化为它的对偶函数最大化的问题。对该问题的拉格朗日函数：

![img](https://syisbest.github.io/svm/8.png)

> 对该式不理解

```
预习不下去了，到这里已经不明白在做什么了
```

## 核函数

## 软间隔与正则化

![img](https://syisbest.github.io/svm/background.jpg)



`作者：沈岩，写于2020.5.19  22:11`

