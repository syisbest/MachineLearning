# 决策树

## 什么是决策树（简单理解）

先看一个经典例子

```
  女儿：多大年纪了？
  母亲：26。
  女儿：长的帅不帅？
  母亲：挺帅的。
  女儿：收入高不？
  母亲：不算很高，中等情况。
  女儿：是公务员不？
  母亲：是，在税务局上班呢。
  女儿：那好，我去见见。
```

画成树的形式就是这样

![img](https://syisbest.github.io/jueceshu/%E5%86%B3%E7%AD%96%E6%A0%91%E7%A4%BA%E4%BE%8B.jpg)

他就和计算机的树结构相似，然后每一个从父节点到子节点都有一个判断条件，来决定往哪个子节点走，不过和传统的说法不一样的是这里对节点的命名是根节点，内部节点和叶子节点。

一般一颗决策树包括一个根节点，若干个内部节点和若干个叶子节点，其中：

- 叶子节点就是不同的类别（当然，会有不同的叶子节点是同一类别），对应决策结果
- 非叶子节点是不同的特征（也就是一个事物的特征，比如例子中的年龄相貌这些来让女孩决定时候去相亲的因素）
- 每个分支代表这个特征属性在某个值域上的输出（也就是决定父节点到他的哪个子节点，例如，帅还是不帅？满足哪一种就到哪一个分支）

> 从直观的例子和图很容易理解思想，但是进入抽象的数据和各种公式推导就难以理解

## 如何构造决策树

西瓜书的算法流程：

![img](https://syisbest.github.io/jueceshu/%E6%9E%84%E5%BB%BA%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95.png)

决策树的生成是一个递归过程，在决策树基本算法中，有三种情形会导致递归返回: 

1. 当前结点包含的样本全属于同一类别，无需划分;
2. 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分; 
3. 当前结点包含的样本集合为空，不能划分.

怎么理解这三种情形呢，加上西瓜例子理解：

1. 第一种，经过了几次递归后的剩下没分的全是好瓜了，那就不用再分它了，给它标上好瓜标签，结束递归;
2. 第二种，属性集没了，也就是虽然我还有数据，但是没属性了，那鬼知道你这个数据是什么瓜，更不能往下划分它了，或者现在传进来的数据集它每个属性都符合，也就是判断不了了;
3. 样本集合空了那不就是已经分好了，分好了我就不往下分了呗.

> 构造决策树的关键在如何选择最优划分属性！！！选的好就好分

那么问题又来了，怎样知道选的好不好呢？用什么来判断，是从结果来看的，如果分出来的结果符合度高，那选的划分属性自然就好。这里引入了一个“纯度”的概念，决策树的分支结点所包含的样本尽可能属于同一类别，结点的"纯度" (purity) 就越来越高。那么量化纯度就能得到我们想要的。

```
我的理解就是量化纯度就能得到某个特征对决策的影响的大小，比如最开始的例子中的女孩最看中年龄，然后是相貌，再然后是收入，而这个只是案例中的女孩的决策标准，那我们要得到的就是有大量数据得到的标准，最看中什么，其次，这样就能得到一个决策树了。
```

## 量化纯度的方法

### `ID3`算法

两个公式

“信息熵”`(information entropy)`是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：

![img](https://syisbest.github.io/jueceshu/Ent.png)

假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）:

![img](https://syisbest.github.io/jueceshu/Gain.png)

第一个公式是由训练集里的结果来计算根节点的信息熵，越小纯度越高。

第二个公式是来算不同属性的信息熵增，每次都让算出来最小的那个做父节点，剩下的属性做他的子节点，算的值相同就做兄弟。

### `C4.5`算法

不用信息增益，而是使用“增益率”

![img](https://syisbest.github.io/jueceshu/Gain_ratio.png)

其中：

![img](https://syisbest.github.io/jueceshu/IV.png)

和前一个方法不同的是，多了一个IV表达式，但a属性的取值越多IV就越大。他用在先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。

### `CART`算法

`CART`决策树使用“基尼指数”`（Gini index）`来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此`Gini(D)`越小越好，基尼指数定义如下：

![img](https://syisbest.github.io/jueceshu/Gini.png)

![img](https://syisbest.github.io/jueceshu/Gini_index.png)

所以选择的时候选择那个使得划分后基尼指数最小的属性作为最优划分属性

## 剪枝

按照训练集构造好了决策树，就很容易出现最常见的问题，过拟合。什么是过拟合呢？比如女孩择偶，我们的训练集里拜金女很多，那完蛋了，得到的决策树里有钱成了第一标准，但是事实肯定不是这样，这个就是过拟合。所以就需要剪枝来对付过拟合。

决策树剪枝的基本策略有"预剪枝" `(prepruning)` 和"后剪枝"`(post-pruning)` 两种：

- 预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点;
- 后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点.

### 预剪枝

用实例图来理解

![img](https://syisbest.github.io/jueceshu/%E6%9C%AA%E5%89%AA%E6%9E%9D%E5%86%B3%E7%AD%96%E6%A0%91.png)

![img](https://syisbest.github.io/jueceshu/%E9%A2%84%E5%89%AA%E6%9E%9D%E5%86%B3%E7%AD%96%E6%A0%91.png)

采用从顶部开始计算精度，如果划分前比划分后精度高，就将之后的节点全部剪掉，这样做确实降低了过拟合的风险，但是这种一刀砍断又会造成欠拟合。

### 后剪枝

同样是用实例图来理解

![img](https://syisbest.github.io/jueceshu/%E5%90%8E%E5%89%AA%E6%9E%9D%E5%86%B3%E7%AD%96%E6%A0%91.png)

从底部的节点开始往根节点来计算，同样是计算剪枝前和之后的精度，如果剪枝后精度变高了，那就剪掉它。这样做从底到顶，就没有一刀切带来的问题了。不过带来的问题是训练时间开销相比预剪枝大大提升。

所以说世间安得双全法啊

![img](https://syisbest.github.io/background_image/mountains-and-hills-5112952_1920.jpg)

> `作者：沈岩，写于2020.5.12 20：03`

