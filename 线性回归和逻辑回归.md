## 线性回归

### 线性回归是什么？

用专业术语来说：线性回归，是利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法。表达形式为`y = wx + e`，e为误差服从均值为0的正态分布。适用于有监督学习的预测。

![img](https://pics2.baidu.com/feed/faedab64034f78f07d9421aa254b8851b2191c4e.jpeg?token=cebcfa517f951b47f51a28e8ff500782&s=3CA67D32E97AE3CE50F52CDA0000C0B1)

我的理解：在简单的二维坐标系里进行理解，就是给了很多数据(x,y)，然后把它们画在坐标纸上，可以找到一条直线，该直线能够尽可能准确地表达这些点x与y的关系，但是除了极其特别的数据，是不能有一条直线能够过每一个点，也就是会有误差。

在二维坐标里只有一个自变量和因变量，于是可以用`y = ax + b`来表达这条直线，这就是一元线性回归。当自变量类型增多后，维度就会升高，此时用`hθ(x)=θ0+θ1X1+...+θnXn`来表达，虽然他不再是直线，但是依旧是线性的，称之为多元线性回归。

那么怎么做能让误差尽量小呢？有这么一个概念：损失函数。

### 损失函数

损失函数是什么呢？它是用来衡量参数选择的准确性，也就是看你的误差大小的函数。

损失函数的公式是
$$
J(θ0,θ1,...,θn)=1/2m∑i=1->m(hθ(x(i))−y(i))^2
$$
很明显，它计算的是线性回归分析的值与实际值的欧式距离的平均值。那么该值越小就说明损失越小，那我的表达式也就越准确。

那怎么来减小它的值呢？观察该函数的表达式，能知道这是一个凸函数，那么我们就能使用梯度下降来减小它的值。

### 梯度下降

梯度下降是逐步最小化损失函数的过程。如果下山的过程，找准了下山方向(梯度)，每次迈进一步，就能直至山地。如果有多个特征，对应多个参数θ,需要对每一个参数做一次迭代。
$$
θj:=θj-α*∂/∂θjJ(θ0,θ1)
$$
这里需要注意的是α的取值，该值就是学习率，能决定下降的快慢，一般取值较小，取0.01。

> 该函数怎么来的呢？？？

## 逻辑回归

### 逻辑回归是什么？

当给的数据不是连续，变量和变量之间也不是线性关系，这些数据呈现出聚集性，也就是能够给结果变量分类，此时就要用到逻辑回归了。

逻辑回归是怎么做的呢？它采用概率的方法，预测出一个概率，来判断结果。

首先进行样本映射，采用`sigmoid`函数，该函数公式：
$$
g(z)=1/(1+e^-z)
$$
曲线：

![img](https://pics6.baidu.com/feed/ac4bd11373f08202b3a78cdc178179e9aa641b9e.jpeg?token=20034a31cfb94d7cbbf903558041a324&s=39843C7249076F5F1451B0CA0000A0B1)

从曲线中可以看到，x值越小，y值就趋近0，x值越大，y值就越趋近1，就把所有的样本都映射到了[0,1]中。

> 那么多维的数据怎么做呢？降维？怎么降？
>
> 哦，原来函数中的z就是多维了，z=![W^{T}X](https://private.codecogs.com/gif.latex?W%5E%7BT%7DX)

### 逻辑回归的具体应用方式

我已经知道，逻辑回归是拿来进行分类，那么它的应用方式是如何的呢？

简单的应用是在二维坐标系中把一组样本用一条直线一分为二，那么在直线一侧的某一点若能求得其概率值小于0.5，就将其判定为负样本。

当然在实际应用中不会简单的取0.5来作为判断标准，该标准以实际情况来选择。

和线性回归一样，逻辑回归也会有误差，那么就同样能有损失函数。

### 损失函数

因为使用`sigmoid`函数使回归的值是0或者1，那么线性回归损失函数的求法就不适用了，有这么一个适合的方法：

![img](https://upload-images.jianshu.io/upload_images/3297229-b7c5fb472c5c5b64.png?imageMogr2/auto-orient/strip|imageView2/2/w/366)

这样就能描绘出y=1和y=0时候的曲线，且符合逻辑回归的误差。即y是正样本时，如果所给的概率很小，此时预测成了负样本，即误差很大，损失就会很大；反之，所给的概率很小，损失就会很小。

![person-walking-on-shore-1665722](E:\guge\person-walking-on-shore-1665722.jpg)

[^]: 作者：沈岩，写于2020.5.5 22:08

